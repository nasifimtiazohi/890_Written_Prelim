# Memory Errors in Security Testing

## Introduction
Software development is an error-prone activity. 
Code written and reviewed by skilled developers 
may end up with various types of bugs 
some of which can be security-critical, i.e. vulnerability. 
Developers use security testing tools (e.g. static analysis, fuzzing) 
in order to identify the potential vulnerabilities. 
However, given the non-perfect nature of these tools 
and the cost of fixing the alerts they generate, 
developers response to such alerts may vary. 
Further, even after extensive security testing, 
undetected vulnerabilities may still remain in code 
that get discovered years after it was introduced.

Memory-related bugs, 
such as buffer overflows and uninitialized reads, 
are an important class of security vulnerabilities 
typically more prevalent in non memory-safe languages such as C, C++. 
Security testing techniques such as static analysis 
can detect a wide range of potential memory issues in the code. 
Tools performing such analysis 
present their results as alerts to the developers. 
However, it is yet to be studied 
how frequently does memory-related alerts are identified by security tools 
and how developers respond to them. 
Further, vulnerabilities that leak through all cautionary testing, 
may get discovered years later. 
If reported to the National Vulnerability Database (NVD),
the discovered vulnerabilities are tracked 
in a central database with a unique identifier called CVE.
However, it is yet to be studied if these vulnerabilities (CVEs) 
appeared in the code because 
security testing techniques failed to identify them or 
the testing could have had indeed identified the flaw but 
the developers did not take a corrective action in time.

In this paper, 
we study 10 C/C++ projects 
that have been using a static analysis security testing tool. 
We analyze the historical scan reports 
generated by the tool for these projects and 
study how frequently memory-related alerts appeared.
We also studied the subsequent developer action on those alerts. 
Moreover, for one of this project, Linux, 
we look at the CVEs that were published and 
investigate if the involved flaw in those vulnerabilities 
were identified by the security tool 
when they were first introduced in the code. 
We state our research questions as:

- **RQ1:** <em>How frequently memory-related alerts are identified by a static analysis security testing tool? 
        How do developers respond to these alerts?</em>
- **RQ2:** <em> How many CVEs were identified by a static analysis security testing tool 
        when the involved flaws were first introduced in the code? </em>

In the following sections,
we first explain our dataset
and then report the findings
followed by a discussion 
on implication and possible threats 
of results presented.

## Dataset
We primarily work with two datasets:
1) An extension of the Coverity dataset 
used in a pior work by Imtiaz et al.[[1]](#1),
2) The CVE dataset from National Vulnerability Database [[2]](#2) 

Alerts in Coverity dataset
and CVEs in NVD database
have corresponding CWE mapping
(Common Weakness Enumeration (CWE)
is a list of software and hardware weaknesses [[3]](#3)).
Two human reviwers independently 
classified the CWE identifiers 
as memory vs. non-memory
to separate out the memory-related issues
in our dataset.

The following subsections explain
the data soources,
and subseuquent CWE classification.

### Coverity Dataset
##### Dataset from prior work:
In our prior work [[1]](#1),
we studied historical scan reports
by a static analysis security testing tool, Coverity,
for five large projects.
Four projects were in C/C++ 
(Linux, Firefox, Samba, Kodi)
and one in Java (Ovirt-engine).
The projects were selected based on -
i) size (at least 100,000 lines of code),
ii) Coverity scan duration and scan interval
(Coverity scan reports available 
for at least past five years
with median scan interval being
less than a week),
iii) access to data 
(project maintainers have grant authors' access to the data),
and iv) developer activity 
(porject maintainers confirmed of their monitoring Coverity reports).

In the Coverity dataset,
for each scan report,
the alerts are tagged as wither
i) Fixed (Coverity stopped detecting the alert in the code,
we will refer to this as <em>eliminated</em>),
ii) Dismissed (When developers have explicitly marked
an alert as <em>False Positive</em> or <em>Intentional</em>),
iii) New (Coverity still detects the alert in the current scan,
we refer to these as <em>alive</em> alerts).

For the eliminated alerts,
to distinguish between **developer fix** 
and other modes of elimination (e.g. alert suppression, file deletion/renaming),
we track any commits made on the affected files for an alert
to see if there were any valid code change made 
when the alert was first marked as fixed by Coverity.
The alerts that were fixed though code change 
made by developers,
we refer to them as <em>actionable</em> alerts.
We also calculated the lifespan of each alert
(the timespan Coverity kept detecting an alert),
and the fix complexity for the actionable alerts
(change in files, lines of code, 
and logical blocks of code
in the commit that fixed the alert).
Below figure explains the workflow.
For more details,
we refer to the original paper [[1]](#1).

![Alt text](drawingfinal.jpg  "Automatically identifying actionable alerts through alert detection
history and affected fileâ€™s commit history")





##### Extension of the dataset for this paper:



### CVE dataset:
CWE classification for memory vs. non-memory:

### CWE classification:
Two reviewers

## Findings

## Discussion
### Implications
### Threats to validity

## Conclusion

# References
<a id="1">[1]</a> 
Imtiaz, Nasif, Brendan Murphy, and Laurie Williams. 
"How Do Developers Act on Static Analysis Alerts? An Empirical Study of Coverity Usage." 
2019 IEEE 30th International Symposium on Software Reliability Engineering (ISSRE). IEEE, 2019.

<a id="2">[2]</a>
https://nvd.nist.gov/vuln/full-listing

<a id="3">[3]</a>
https://cwe.mitre.org/